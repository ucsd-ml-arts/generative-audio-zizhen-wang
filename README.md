# Project 3 Generative Audio

Zizhen Wang, ziw142@ucsd.edu



## Abstract

Different types music always have the specific rhythm, melody and instruments. People could recognize these sytles depends on the them but if we use machine learning to generate the new music which it may change the number of instrument, the melody and the rhythm, could we still can recognize them. I will sue GANsynth, MusicVAE and Transformer to change the number of instruments, the melody and the rhythm in different orders to get results.

## Model/Data


La companella
This is a piano song which represents classical style and only one instrument.



## Code
Jupyter notebooks: 
- generative_code.ipynb [gansynth_external.ipynb](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/gansynth_external.ipynb)
- Generating Piano Music with Transformer.ipynb [Generating_Piano_Music_with_Transformer.ipynb](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/Generating_Piano_Music_with_Transformer.ipynb)
- MusicVAE.ipynb [MusicVAE.ipynb](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/MusicVAE.ipynb)

## Results
- Firstly, I used GANSynth generator to generate new music by 16 random samples of instruments. GANSynth is a new approach to audio synthesis using neural networks to release a playable set of neural synthsizer instruments. The Synth dataset has a lot of intruments samples and qualities like bright or dark.  They use encoder and decoder to generate new instuments into new waveform. The generate custom inerpolation for instruments: [0, 3, 6, 0] and for time :[0, 0.3, 0.6, 1.0]
![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/01.jpg) ![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/02.jpg)
- [03 - La Campanella.mp3](https://www.driveplayer.com/#fileIds=1uJ7OjY9J_JI7a-0ljdvY03kuDINwl8_7&userId={userId})
- [generated_clip.wav](https://www.driveplayer.com/#fileIds=1wes7C-NwRwIrbI1a45EJgnVIOejcnAdv&userId={userId}) You could also download from github if you do not have permission.
- The first one is original music and second one is generated music. We will see the form for x-axis is almost same which means the rhythm and melody is almost same. We could also listen from these 2 musics. The interesting thing is the generated music still keep the classic music style and it listens like symphony or drama which have a lot of instruments.

- Secondly, I used MusicVAE for 16-bar Melody Models. MusicVAE is a hierarchical recurrent variational autoencoder for learning latent spaces for musical scores.. For the training data, I both use the original music and generated music by GANSynth to input 16-bar melody models and temperature is 0.5.
![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/03.jpg) ![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/04.jpg)
- [hierdec_mel_16bar_mean.mp3](https://www.driveplayer.com/#fileIds=1RWU81FDX85-TUaEZ3KXS5KQJ0IAsuJro&userId={userId})
- [hierdec_mel_16bar_mean2.mp3](https://www.driveplayer.com/#fileIds=1RqNkG644XDAfcey46_jTpsk7ekVPqLod&userId=104858041665593101824)  You could also download from github if you do not have permission.
- The first one is generated by original music and the second one is generated by generated music by GANSynth. These two we see that they both changed the melody. Since the provided samples are all piano so they both generated output is piano music. However, the first one is more discrete in waveform and the second is more continuous in waveform. After listening these two samples, we found the first one has like more staccato and the second one has more prolonged sound and more bass sound. My assumption is more instruments have more waveforms covered and when it decodes and encodes, the output will be more smooth and continuous.


- Thirdly, I used Generating Piano Music with Transformer for melody-conditioned piano performance model. Music Transformer is an attention-based neural network that can generate music with improved long-term coherence. For the training data, I both use the original music and generated music by MusicVAE and temperature is 0.5.
![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/05.jpg) -![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/06.jpg)
- [accompaniment.mp3](https://www.driveplayer.com/#fileIds=1Yh0O-HFq4Ui8o9R72_1-ppnbubb_0AfP&userId=104858041665593101824)
- [accompaniment2.mp3](https://www.driveplayer.com/#fileIds=1zYI-0k0NzUsWcf7XqOkVG3aAjngXQp8j&userId=104858041665593101824)

- The first one is generated by mean1 music and the second one is generated by mean2 mucic as before. These two we see that they both changed the rhythm is much more than melody so we could still listen the similar melody compared with the last part. Since the provided samples are all piano so they both generated output is piano music. However, the first one is rapid which the rhythm is get faster and the second one has more parts. My assumption is more instruments have more waveforms covered and when it decodes and encodes, the output will be multi-voice music which have more parts.

- Lastly, I used GANSynth generator to generate accompaniment2 to compare with the generated music from the first part.
![](https://github.com/ucsd-ml-arts/generative-audio-zizhen-wang/blob/master/07.jpg)
- [generated_clip2.wav](https://www.driveplayer.com/#fileIds=1aV2jFeEvVJtTPUv3nJKUmaIJ90gAJYdW&userId=104858041665593101824)

This new music which all the components are changed that we could not recognize the classical style. This music is like a new type electrical music which is very interesting.

## Technical Notes



## Reference
- Music Transformer: Generating Music with Long-Term Structure
https://magenta.tensorflow.org/music-transformer
- MusicVAE: Creating a palette for musical scores with machine learning.
https://magenta.tensorflow.org/music-vae
- GANSynth: Making music with GANs
https://magenta.tensorflow.org/gansynth
